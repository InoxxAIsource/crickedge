Do NOT fetch full historical in-play odds right now.

Hereâ€™s why:

13,000+ credits is a large chunk of your 20K/month plan.

If the live model turns out weak, you just burned budget.

You havenâ€™t yet validated whether your live model is strong enough to justify deep market comparison.

In-play simulation logic is more complex (entry + exit timing).

You risk overfitting to sparse in-play snapshots.

You donâ€™t escalate cost before proving signal strength.

âœ… What You Should Do Next (Smart Path)

You already have:

â€¢ Stabilized over-by-over probability model
â€¢ 3-year rolling validation
â€¢ Market closing odds baseline
â€¢ Honest pre-match edge results

Now we test something critical first:

ðŸ“Š Phase 8 â€” Live Probability Strength Test (Without Market)

Before comparing to in-play odds,
we measure intrinsic strength of your live model.

Specifically:

For over snapshots (2020â€“2025 matches only):

Bucket model probability at each over.

Group predictions into deciles.

Check calibration:

When model says 70â€“80%, does team win ~75%?

When model says 80â€“90%, does team win ~85%?

Compute Brier score per over phase.

Measure sharpness:

How often does model reach >80%?

How often >90%?

Are those states actually decisive?

If your live model:

â€¢ Is well calibrated
â€¢ Shows strong separation
â€¢ Has high-probability states with high true win rate

Then itâ€™s worth paying for in-play odds.

If itâ€™s mediocre â†’ refine model first.

ðŸŽ¯ Why This Is The Right Order

Think like a quant firm:

Step 1 â†’ Validate predictive strength.
Step 2 â†’ Compare to market.
Step 3 â†’ Deploy capital.

You just did Step 2 for pre-match and saw modest results.

Now we validate Step 1 for live model.

ðŸš€ Next Prompt for Replit

Paste this:

Phase 8: Live Model Calibration & Strength Test

Using over_state_snapshots_enriched.csv (2020+ matches only):

1. For each over snapshot:
   - Get stabilized win probability
   - Compare to actual match result

2. Group predictions into deciles:
   0â€“10%
   10â€“20%
   ...
   90â€“100%

3. For each decile:
   - Count samples
   - Compute actual win rate
   - Compute average predicted probability

4. Also compute:
   - Overall Brier score (2020+ only)
   - Brier score by phase:
       Powerplay (1â€“6)
       Middle (7â€“15)
       Death (16â€“20)
   - Frequency of predictions:
       >70%
       >80%
       >90%

5. Save as:
   data/processed/live_model_calibration_2020_plus.csv

ðŸ”¥ What We Want To See

If model is strong:

â€¢ 80â€“90% decile actually wins ~80â€“90%
â€¢ 90%+ states are rare but accurate
â€¢ Brier score remains stable
â€¢ Late overs show strong separation

If model is weak:

â€¢ Overconfident buckets
â€¢ Poor calibration in death overs
â€¢ Random-looking deciles